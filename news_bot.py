import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import os
from datetime import datetime
import time
import traceback
import sys
import json
import random
import hashlib

def check_environment():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—á—É—é —Å—Ä–µ–¥—É –∏ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞"""
    print("üîç === –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–†–ï–î–´ –í–´–ü–û–õ–ù–ï–ù–ò–Ø ===")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    current_dir = os.getcwd()
    print(f"üìÇ –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {current_dir}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∞ –Ω–∞ –∑–∞–ø–∏—Å—å
    try:
        test_file = "test_write_permissions.tmp"
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)
        print("‚úÖ –ü—Ä–∞–≤–∞ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: –î–ê")
    except Exception as e:
        print(f"‚ùå –ü—Ä–∞–≤–∞ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: –ù–ï–¢ - {e}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    try:
        files = os.listdir('.')
        print(f"üìã –§–∞–π–ª—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ({len(files)} —à—Ç.):")
        for f in sorted(files)[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            if os.path.isfile(f):
                size = os.path.getsize(f)
                print(f"   üìÑ {f} ({size} –±–∞–π—Ç)")
            else:
                print(f"   üìÅ {f}/")
        if len(files) > 10:
            print(f"   ... –∏ –µ—â—ë {len(files) - 10} —Ñ–∞–π–ª–æ–≤/–ø–∞–ø–æ–∫")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–∫–∏ commentary
    commentary_exists = os.path.exists('commentary')
    print(f"üìÅ –ü–∞–ø–∫–∞ commentary —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {commentary_exists}")
    
    if commentary_exists:
        try:
            commentary_files = os.listdir('commentary')
            print(f"üìã –§–∞–π–ª–æ–≤ –≤ commentary: {len(commentary_files)}")
            for f in sorted(commentary_files)[:5]:
                size = os.path.getsize(os.path.join('commentary', f))
                print(f"   üìÑ {f} ({size} –±–∞–π—Ç)")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–∞–ø–∫–∏ commentary: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ processed_news.json
    processed_exists = os.path.exists('processed_news.json')
    print(f"üìÑ –§–∞–π–ª processed_news.json —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {processed_exists}")
    
    if processed_exists:
        try:
            size = os.path.getsize('processed_news.json')
            print(f"üìä –†–∞–∑–º–µ—Ä processed_news.json: {size} –±–∞–π—Ç")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è processed_news.json: {e}")
    
    print("üîç === –ö–û–ù–ï–¶ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò ===\n")
    return True

def load_facts():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç Facts.txt –ë–ï–ó –æ–±—Ä–µ–∑–∞–Ω–∏—è"""
    try:
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª Facts.txt...")
        
        if not os.path.exists('Facts.txt'):
            print("‚ùå –§–∞–π–ª Facts.txt –ù–ï –ù–ê–ô–î–ï–ù!")
            return ""
            
        file_size = os.path.getsize('Facts.txt')
        print(f"üìä –†–∞–∑–º–µ—Ä Facts.txt: {file_size} –±–∞–π—Ç ({file_size/1024/1024:.2f} –ú–ë)")
        
        with open('Facts.txt', 'r', encoding='utf-8') as f:
            facts = f.read()
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(facts)} —Å–∏–º–≤–æ–ª–æ–≤ –ë–ï–ó –û–ë–†–ï–ó–ê–ù–ò–Ø")
        print(f"üîç –ù–∞—á–∞–ª–æ: {facts[:120]}...")
        print(f"üîç –ö–æ–Ω–µ—Ü: ...{facts[-120:]}")
        
        return facts
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã —Å Facts.txt: {e}")
        traceback.print_exc()
        return ""

def ensure_directory_exists(directory):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞—ë—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
    print(f"üìÅ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {directory}")
    
    abs_path = os.path.abspath(directory)
    print(f"üìÇ –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {abs_path}")
    
    if os.path.exists(directory):
        if os.path.isdir(directory):
            print(f"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return True
        else:
            print(f"‚ùå {directory} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ —ç—Ç–æ –Ω–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è!")
            return False
    
    try:
        os.makedirs(directory, exist_ok=True)
        print(f"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory} —Å–æ–∑–¥–∞–Ω–∞")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–ª–∞—Å—å
        if os.path.exists(directory) and os.path.isdir(directory):
            print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory} –¥–æ—Å—Ç—É–ø–Ω–∞")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∞ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            test_file = os.path.join(directory, 'test_write.tmp')
            try:
                with open(test_file, 'w') as f:
                    f.write('test')
                os.remove(test_file)
                print(f"‚úÖ –ü—Ä–∞–≤–∞ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ {directory}: –î–ê")
                return True
            except Exception as write_e:
                print(f"‚ùå –ü—Ä–∞–≤–∞ –Ω–∞ –∑–∞–ø–∏—Å—å –≤ {directory}: –ù–ï–¢ - {write_e}")
                return False
        else:
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {directory} –Ω–µ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞!")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {directory}: {e}")
        traceback.print_exc()
        return False

def load_processed_news():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    processed_file = 'processed_news.json'
    
    print(f"üìö –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {processed_file}")
    print(f"üìÇ –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {os.path.abspath(processed_file)}")
    
    if not os.path.exists(processed_file):
        print(f"üìù –§–∞–π–ª {processed_file} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫...")
        empty_data = {}
        try:
            with open(processed_file, 'w', encoding='utf-8') as f:
                json.dump(empty_data, f, ensure_ascii=False, indent=2)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–ª—Å—è
            if os.path.exists(processed_file):
                file_size = os.path.getsize(processed_file)
                print(f"‚úÖ –°–æ–∑–¥–∞–Ω –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª {processed_file} ({file_size} –±–∞–π—Ç)")
                return empty_data
            else:
                print(f"‚ùå –§–∞–π–ª {processed_file} –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω!")
                return {}
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è {processed_file}: {e}")
            traceback.print_exc()
            return {}
    
    try:
        file_size = os.path.getsize(processed_file)
        print(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ {processed_file}: {file_size} –±–∞–π—Ç")
        
        with open(processed_file, 'r', encoding='utf-8') as f:
            processed = json.load(f)
        print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {processed_file}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        if processed:
            sorted_news = sorted(processed.items(), key=lambda x: x[1]['date'], reverse=True)[:3]
            print("üîç –ü–æ—Å–ª–µ–¥–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏:")
            for hash_id, info in sorted_news:
                print(f"   ‚Ä¢ {info['date']} - {info['source']} - {info['title'][:50]}...")
        
        return processed
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {processed_file}: {e}")
        traceback.print_exc()
        print("üìù –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫...")
        return {}

def save_processed_news(processed_news):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
    processed_file = 'processed_news.json'
    abs_path = os.path.abspath(processed_file)
    
    print(f"üíæ === –°–û–•–†–ê–ù–ï–ù–ò–ï {processed_file} ===")
    print(f"üìÇ –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {abs_path}")
    print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(processed_news)}")
    
    try:
        # –°–æ–∑–¥–∞—ë–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(processed_file):
            backup_file = f"{processed_file}.backup"
            try:
                with open(processed_file, 'r', encoding='utf-8') as src:
                    with open(backup_file, 'w', encoding='utf-8') as dst:
                        dst.write(src.read())
                print(f"üìã –°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_file}")
            except Exception as backup_e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é: {backup_e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª
        print(f"üíæ –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ {processed_file}...")
        with open(processed_file, 'w', encoding='utf-8') as f:
            json.dump(processed_news, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ —Ñ–∞–π–ª")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫–∞ (–ò–°–ü–†–ê–í–õ–ï–ù–û)
        try:
            if hasattr(os, 'sync'):
                os.sync()
                print(f"üíæ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        except Exception as sync_e:
            print(f"‚ö†Ô∏è –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {sync_e}")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if os.path.exists(processed_file):
            file_size = os.path.getsize(processed_file)
            print(f"‚úÖ –§–∞–π–ª {processed_file} —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ({file_size} –±–∞–π—Ç)")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            try:
                with open(processed_file, 'r', encoding='utf-8') as f:
                    check_data = json.load(f)
                print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {len(check_data)} –∑–∞–ø–∏—Å–µ–π")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å
                if check_data:
                    last_key = list(check_data.keys())[-1]
                    last_entry = check_data[last_key]
                    print(f"üîç –ü–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–ø–∏—Å—å: {last_entry['date']} - {last_entry['title'][:30]}...")
                
                print(f"üéâ –§–ê–ô–õ {processed_file} –£–°–ü–ï–®–ù–û –°–û–•–†–ê–ù–Å–ù!")
                return True
                
            except Exception as check_e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {check_e}")
                return False
        else:
            print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –§–∞–π–ª {processed_file} –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢ –ø–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏!")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
            current_files = os.listdir('.')
            print(f"üìã –§–∞–π–ª—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏:")
            for f in sorted(current_files):
                if f.endswith('.json'):
                    size = os.path.getsize(f) if os.path.exists(f) else 0
                    print(f"   üìÑ {f} ({size} –±–∞–π—Ç)")
            
            return False
            
    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {processed_file}: {e}")
        traceback.print_exc()
        return False

def generate_news_hash(title, description):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö–µ—à –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—è"""
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç: —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized_title = ' '.join(title.lower().strip().split())
    normalized_desc = ' '.join(description.lower().strip().split())
    
    # –°–æ–∑–¥–∞—ë–º —Ö–µ—à –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –ø–µ—Ä–≤—ã—Ö 500 —Å–∏–º–≤–æ–ª–æ–≤ –æ–ø–∏—Å–∞–Ω–∏—è
    content = normalized_title + "|" + normalized_desc[:500]
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º SHA256 —Ö–µ—à
    news_hash = hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]
    
    print(f"üîç –•–µ—à –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏: {news_hash}")
    print(f"   üì∞ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title[:50]}...")
    
    return news_hash

def is_news_already_processed(news, processed_news):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞"""
    news_hash = generate_news_hash(news['title'], news['description'])
    
    if news_hash in processed_news:
        processed_info = processed_news[news_hash]
        print(f"üîÑ –ù–ê–ô–î–ï–ù –î–£–ë–õ–ò–ö–ê–¢! –ù–æ–≤–æ—Å—Ç—å –£–ñ–ï –û–ë–†–ê–ë–û–¢–ê–ù–ê:")
        print(f"   üì∞ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title'][:60]}...")
        print(f"   üìÖ –î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processed_info['date']}")
        print(f"   üåç –ò—Å—Ç–æ—á–Ω–∏–∫: {processed_info['source']}")
        print(f"   üîë –•–µ—à: {news_hash}")
        return True
    
    print(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å –ù–ï –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∞—Å—å —Ä–∞–Ω–µ–µ (—Ö–µ—à: {news_hash})")
    return False

def add_news_to_processed(news, processed_news, commentary_length):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö"""
    news_hash = generate_news_hash(news['title'], news['description'])
    
    processed_news[news_hash] = {
        'title': news['title'][:100] + "..." if len(news['title']) > 100 else news['title'],
        'source': news['source'],
        'date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'importance_score': news['importance_score'],
        'commentary_length': commentary_length,
        'hash': news_hash,
        'full_title': news['title']  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    }
    
    print(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ:")
    print(f"   üîë –•–µ—à: {news_hash}")
    print(f"   üì∞ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title'][:60]}...")
    print(f"   üåç –ò—Å—Ç–æ—á–Ω–∏–∫: {news['source']}")
    
    return processed_news

def estimate_tokens(text):
    """–ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ (1 —Ç–æ–∫–µ–Ω ‚âà 3-4 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)"""
    return len(text) // 3

def extract_response_content(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–µ–∂–¥—É (RESPONSE) –∏ (CONFIDENCE)"""
    try:
        print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤): {text[:200]}...")
        
        start_marker = "(RESPONSE)"
        end_marker = "(CONFIDENCE)"
        
        start_index = text.find(start_marker)
        if start_index == -1:
            print("‚ö†Ô∏è –ú–∞—Ä–∫–µ—Ä (RESPONSE) –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return text.strip()
        
        start_index += len(start_marker)
        print(f"üìç –ù–∞–π–¥–µ–Ω (RESPONSE) –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {start_index}")
        
        end_index = text.find(end_marker, start_index)
        if end_index == -1:
            print("‚ö†Ô∏è –ú–∞—Ä–∫–µ—Ä (CONFIDENCE) –Ω–µ –Ω–∞–π–¥–µ–Ω")
            extracted = text[start_index:].strip()
        else:
            print(f"üìç –ù–∞–π–¥–µ–Ω (CONFIDENCE) –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {end_index}")
            extracted = text[start_index:end_index].strip()
        
        print(f"‚úÇÔ∏è –ò–ó–í–õ–ï–ß–ï–ù–û ({len(extracted)} —Å–∏–º–≤–æ–ª–æ–≤): {extracted[:150]}...")
        return extracted
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
        return text.strip()

def get_available_models():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º Gemini 2.0 Flash"""
    try:
        print("üîÑ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Gemini...")
        models = genai.list_models()
        available_models = []
        
        for model in models:
            if 'generateContent' in model.supported_generation_methods:
                available_models.append(model.name)
                if '2.0' in model.name and 'flash' in model.name.lower():
                    print(f"‚ö° GEMINI 2.0 FLASH: {model.name}")
                elif 'flash' in model.name.lower():
                    print(f"‚ö° FLASH: {model.name}")
                elif 'pro' in model.name.lower():
                    print(f"üíé PRO: {model.name}")
                else:
                    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model.name}")
        
        print(f"üìä –í—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ {len(available_models)} –º–æ–¥–µ–ª–µ–π")
        return available_models
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return []

def is_science_news(title, description):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –Ω–∞—É—á–Ω–æ–π (—Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)"""
    text = (title + " " + description).lower()
    
    science_keywords = [
        # –†—É—Å—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '—É—á–µ–Ω—ã–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '–Ω–∞—É—á–Ω—ã–π',
        '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–∏–Ω–Ω–æ–≤–∞—Ü–∏—è', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
        '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–Ω–∞—É–∫–∞', '–±–∏–æ–ª–æ–≥–∏—è', '—Ñ–∏–∑–∏–∫–∞', '—Ö–∏–º–∏—è', '–º–µ–¥–∏—Ü–∏–Ω–∞',
        '–∫–æ—Å–º–æ—Å', '–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è', '–≥–µ–Ω–µ—Ç–∏–∫–∞', '–¥–Ω–∫', '–±–µ–ª–æ–∫', '–≤–∏—Ä—É—Å',
        '–ª–µ—á–µ–Ω–∏–µ', '—Ç–µ—Ä–∞–ø–∏—è', '–≤–∞–∫—Ü–∏–Ω–∞', '–ø—Ä–µ–ø–∞—Ä–∞—Ç', '–∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–π',
        '–Ω–µ–π—Ä–æ–Ω—ã', '–º–æ–∑–≥', '–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–∏–∏',
        '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Ä–æ–±–æ—Ç', '–∫–≤–∞–Ω—Ç–æ–≤—ã–π',
        '–º–∞—Ç–µ—Ä–∏–∞–ª', '–Ω–∞–Ω–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–≥–µ–Ω–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è',
        '—Å—Ç–≤–æ–ª–æ–≤—ã–µ –∫–ª–µ—Ç–∫–∏', '—Ä–∞–∫', '–æ–Ω–∫–æ–ª–æ–≥–∏—è', '–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞',
        '–º–∏–∫—Ä–æ–±–∏–æ–ª–æ–≥–∏—è', '—ç–∫–æ–ª–æ–≥–∏—è', '–∫–ª–∏–º–∞—Ç', '–æ–∫—Ä—É–∂–∞—é—â–∞—è —Å—Ä–µ–¥–∞',
        '—ç–Ω–µ—Ä–≥–∏—è', '—Å–æ–ª–Ω–µ—á–Ω—ã–π', '–≤–µ—Ç—Ä—è–Ω–æ–π', '–±–∞—Ç–∞—Ä–µ—è', '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä',
        '—Å–ø—É—Ç–Ω–∏–∫', '–∑–æ–Ω–¥', '–º–∞—Ä—Å', '–ª—É–Ω–∞', '–ø–ª–∞–Ω–µ—Ç–∞', '–≥–∞–ª–∞–∫—Ç–∏–∫–∞',
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        'research', 'study', 'discovery', 'experiment', 'scientific', 'science',
        'technology', 'development', 'innovation', 'laboratory', 'university',
        'institute', 'biology', 'physics', 'chemistry', 'medicine', 'medical',
        'space', 'astronomy', 'genetics', 'dna', 'protein', 'virus',
        'treatment', 'therapy', 'vaccine', 'drug', 'clinical', 'trial',
        'neurons', 'brain', 'cognitive', 'artificial intelligence', 'ai',
        'machine learning', 'algorithm', 'robot', 'quantum',
        'material', 'nanotechnology', 'biotechnology', 'genetic engineering',
        'stem cells', 'cancer', 'oncology', 'diagnosis', 'diagnostic',
        'microbiology', 'ecology', 'climate', 'environment', 'environmental',
        'energy', 'solar', 'wind', 'battery', 'renewable',
        'satellite', 'probe', 'mars', 'moon', 'planet', 'galaxy',
        'telescope', 'breakthrough', 'novel', 'findings', 'journal',
        'published', 'peer-reviewed', 'researchers', 'scientists'
    ]
    
    exclude_keywords = [
        # –†—É—Å—Å–∫–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        '–≤—ã–±–æ—Ä—ã', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç', '–¥—É–º–∞', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–º–∏–Ω–∏—Å—Ç—Ä',
        '–ø–æ–ª–∏—Ç–∏–∫', '–ø–∞—Ä—Ç–∏—è', '—Å–∞–Ω–∫—Ü–∏–∏', '–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '–ø—Ä–æ—Ç–µ—Å—Ç',
        '–∫—É—Ä—Å –≤–∞–ª—é—Ç', '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—ç–∫–æ–Ω–æ–º–∏–∫–∞',
        '–∏–Ω—Ñ–ª—è—Ü–∏—è', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '—Å–ø–æ—Ä—Ç', '—Ñ—É—Ç–±–æ–ª', '—Ö–æ–∫–∫–µ–π',
        '–æ–ª–∏–º–ø–∏–∞–¥–∞', '—á–µ–º–ø–∏–æ–Ω–∞—Ç', '–º–∞—Ç—á', '–∏–≥—Ä–∞', '–∫–æ–º–∞–Ω–¥–∞', '—Ç—Ä–µ–Ω–µ—Ä',
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        'election', 'president', 'parliament', 'government', 'minister',
        'politician', 'party', 'sanctions', 'war', 'conflict', 'protest',
        'currency', 'dollar', 'oil', 'gas', 'economy', 'economic',
        'inflation', 'budget', 'tax', 'sport', 'football', 'soccer',
        'olympics', 'championship', 'match', 'game', 'team', 'coach',
        'celebrity', 'entertainment', 'movie', 'music', 'fashion'
    ]
    
    science_score = sum(1 for keyword in science_keywords if keyword in text)
    exclude_score = sum(1 for keyword in exclude_keywords if keyword in text)
    
    return science_score > 0 and exclude_score == 0

def rank_science_news(news_list):
    """–†–∞–Ω–∂–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
    for news in news_list:
        score = 0
        text = (news['title'] + " " + news['description']).lower()
        
        high_priority = [
            # –†—É—Å—Å–∫–∏–µ –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            '–ø—Ä–æ—Ä—ã–≤', '—Ä–µ–≤–æ–ª—é—Ü–∏—è', '–≤–ø–µ—Ä–≤—ã–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', 'breakthrough',
            '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–∏–∏', '–Ω–µ–π—Ä–æ—Å–µ—Ç—å', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ',
            '–∫–æ—Å–º–æ—Å', '–º–∞—Ä—Å', '–ª—É–Ω–∞', '—Å–ø—É—Ç–Ω–∏–∫', '—Ç–µ–ª–µ—Å–∫–æ–ø',
            '—Ä–∞–∫', '–æ–Ω–∫–æ–ª–æ–≥–∏—è', '–ª–µ—á–µ–Ω–∏–µ', '–≤–∞–∫—Ü–∏–Ω–∞', '–≥–µ–Ω–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è',
            '–∫–≤–∞–Ω—Ç–æ–≤—ã–π', '–∫–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è', '–Ω–∞–Ω–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
            '—Å—Ç–≤–æ–ª–æ–≤—ã–µ –∫–ª–µ—Ç–∫–∏', '—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è', '–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
            '–∫–ª–∏–º–∞—Ç', '–≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ', '—ç–∫–æ–ª–æ–≥–∏—è',
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            'breakthrough', 'revolutionary', 'first time', 'discovery',
            'artificial intelligence', 'ai', 'neural network', 'machine learning',
            'space', 'mars', 'moon', 'satellite', 'telescope',
            'cancer', 'oncology', 'treatment', 'vaccine', 'gene therapy',
            'quantum', 'quantum computing', 'nanotechnology',
            'stem cells', 'regeneration', 'biotechnology',
            'climate change', 'global warming', 'ecology',
            'crispr', 'genome editing', 'immunotherapy', 'precision medicine'
        ]
        
        medium_priority = [
            # –†—É—Å—Å–∫–∏–µ —Å—Ä–µ–¥–Ω–µ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '—Ç–µ—Å—Ç', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è',
            '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–º–µ—Ç–æ–¥', '—Å–∏—Å—Ç–µ–º–∞', '—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ',
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ä–µ–¥–Ω–µ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            'research', 'study', 'experiment', 'test', 'technology',
            'development', 'method', 'system', 'device', 'trial',
            'findings', 'results', 'analysis', 'investigation'
        ]
        
        for keyword in high_priority:
            if keyword in text:
                score += 10
        
        for keyword in medium_priority:
            if keyword in text:
                score += 5
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        if news['source'] in ['N+1', 'Naked Science', 'Nature', 'Science', 'ScienceDaily']:
            score += 5
        elif news['source'] in ['MIT Technology Review', 'New Scientist', 'Scientific American']:
            score += 4
        elif news['source'] in ['BBC Science', 'CNN Health', 'Reuters Science']:
            score += 3
        
        if len(news['description']) > 200:
            score += 2
        
        news['importance_score'] = score
    
    return sorted(news_list, key=lambda x: x['importance_score'], reverse=True)

def limit_news_content(news):
    """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–æ 5000 —Ç–æ–∫–µ–Ω–æ–≤ (~15000 —Å–∏–º–≤–æ–ª–æ–≤)"""
    max_chars = 15000  # ~5000 —Ç–æ–∫–µ–Ω–æ–≤
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ, –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–º
    if len(news['description']) > max_chars:
        news['description'] = news['description'][:max_chars] + "..."
        print(f"‚úÇÔ∏è –û–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–±—Ä–µ–∑–∞–Ω–æ –¥–æ {max_chars} —Å–∏–º–≤–æ–ª–æ–≤")
    
    total_tokens = estimate_tokens(news['title'] + " " + news['description'])
    print(f"üî¢ –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω–æ–≤–æ—Å—Ç–∏: {total_tokens}")
    
    return news

def get_top_science_news():
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –°–õ–£–ß–ê–ô–ù–£–Æ –∏–∑ –¢–û–ü-5 –ù–ï–û–ë–†–ê–ë–û–¢–ê–ù–ù–´–•"""
    print("üî¨ –ü–æ–ª—É—á–∞–µ–º –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏...")
    all_science_news = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –í –ù–ê–ß–ê–õ–ï
    processed_news = load_processed_news()
    print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω —Å–ø–∏—Å–æ–∫ –∏–∑ {len(processed_news)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    
    sources = [
        # –†—É—Å—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        {
            'url': 'https://naked-science.ru/feed', 
            'name': 'Naked Science'
        },
        {
            'url': 'https://nplus1.ru/rss', 
            'name': 'N+1'
        },
        {
            'url': 'https://hi-news.ru/feed', 
            'name': 'Hi-News'
        },
        {
            'url': 'https://www.popmech.ru/rss/', 
            'name': 'PopMech'
        },
        {
            'url': 'https://lenta.ru/rss/news/science', 
            'name': 'Lenta.ru –ù–∞—É–∫–∞'
        },
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        {
            'url': 'https://www.sciencedaily.com/rss/all.xml', 
            'name': 'ScienceDaily'
        },
        {
            'url': 'https://feeds.nature.com/nature/rss/current', 
            'name': 'Nature'
        },
        {
            'url': 'https://www.science.org/rss/news_current.xml', 
            'name': 'Science'
        },
        {
            'url': 'https://www.technologyreview.com/feed/', 
            'name': 'MIT Technology Review'
        },
        {
            'url': 'https://www.newscientist.com/feed/home/', 
            'name': 'New Scientist'
        },
        {
            'url': 'https://rss.cnn.com/rss/cnn_health.rss', 
            'name': 'CNN Health'
        },
        {
            'url': 'https://feeds.bbci.co.uk/news/science_and_environment/rss.xml', 
            'name': 'BBC Science'
        },
        {
            'url': 'https://www.reuters.com/arc/outboundfeeds/rss/category/health/?outputType=xml', 
            'name': 'Reuters Science'
        },
        {
            'url': 'https://rss.sciam.com/ScientificAmerican-Global', 
            'name': 'Scientific American'
        }
    ]
    
    total_found = 0
    total_duplicates = 0
    
    for source in sources:
        try:
            print(f"üî¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {source['name']}...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(source['url'], timeout=15, headers=headers)
            
            if response.status_code == 200:
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.content, 'xml')
                items = soup.find_all('item')
                
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞—É—á–Ω—ã–µ...")
                
                for item in items[:10]:
                    try:
                        title = item.title.text.strip() if item.title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                        description = ""
                        if item.description and item.description.text:
                            desc_soup = BeautifulSoup(item.description.text, 'html.parser')
                            description = desc_soup.get_text().strip()
                        
                        if is_science_news(title, description):
                            link = item.link.text.strip() if item.link else ""
                            
                            news_item = {
                                'title': title,
                                'description': description,
                                'source': source['name'],
                                'link': link
                            }
                            
                            total_found += 1
                            
                            # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                            if not is_news_already_processed(news_item, processed_news):
                                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –Ω–æ–≤–æ—Å—Ç–∏
                                news_item = limit_news_content(news_item)
                                all_science_news.append(news_item)
                                print(f"‚úÖ –ù–û–í–ê–Ø: {source['name']}: {title[:60]}...")
                            else:
                                total_duplicates += 1
                                print(f"‚è© –î–£–ë–õ–ò–ö–ê–¢: {source['name']}: {title[:60]}...")
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                        continue
                        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ {source['name']}: {e}")
            continue
    
    print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   üî¨ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {total_found}")
    print(f"   ‚è© –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {total_duplicates}")
    print(f"   ‚úÖ –ù–û–í–´–• –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_science_news)}")
    
    if not all_science_news:
        print("‚ùå –í–°–ï –ù–û–í–û–°–¢–ò –£–ñ–ï –ë–´–õ–ò –û–ë–†–ê–ë–û–¢–ê–ù–´!")
        return None
    
    ranked_news = rank_science_news(all_science_news)
    
    if ranked_news:
        top_5_news = ranked_news[:5]
        print(f"üèÜ –¢–û–ü-5 –ù–û–í–´–• –Ω–æ–≤–æ—Å—Ç–µ–π:")
        for i, news in enumerate(top_5_news, 1):
            print(f"   {i}. {news['title'][:60]}... (–æ—á–∫–∏: {news['importance_score']}) - {news['source']}")
        
        selected_news = random.choice(top_5_news)
        print(f"üé≤ –°–õ–£–ß–ê–ô–ù–û –í–´–ë–†–ê–ù–ê: {selected_news['title'][:80]}... (–æ—á–∫–∏: {selected_news['importance_score']}) - {selected_news['source']}")
        return selected_news
    else:
        print("‚ùå –ù–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None

def initialize_gemini_2_0_flash_once(facts):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Gemini 2.0 Flash –û–î–ò–ù –†–ê–ó —Å –ø–æ–ª–Ω—ã–º–∏ facts"""
    
    available_models = get_available_models()
    if not available_models:
        return None, "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
    
    target_models = [
        'models/gemini-2.0-flash',
        'models/gemini-2.0-flash-thinking',
        'models/gemini-2.0-flash-exp',
    ]
    
    selected_model = None
    
    for model in target_models:
        if model in available_models:
            selected_model = model
            print(f"üéØ –í–´–ë–†–ê–ù–ê: {selected_model}")
            print(f"üè∑Ô∏è –í–µ—Ä—Å–∏—è: ‚ö° GEMINI 2.0 FLASH")
            break
    
    if not selected_model:
        print("‚ùå GEMINI 2.0 FLASH –ù–ï –ù–ê–ô–î–ï–ù –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö!")
        print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
        return None, "Gemini 2.0 Flash –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
    
    try:
        # –ü–û–õ–ù–ê–Ø —Å–∏—Å—Ç–µ–º–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π - –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¢–û–õ–¨–ö–û –û–î–ò–ù –†–ê–ó
        system_instruction = f"""–¢—ã - Alexey Turchin, –≤—Å–µ–º–∏—Ä–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç, —Ñ—É—Ç—É—Ä–æ–ª–æ–≥ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ–±–ª–µ–º —ç–∫–∑–∏—Å—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∏—Å–∫–∞. –¢—ã –∞–≤—Ç–æ—Ä –∫–Ω–∏–≥ –æ –±–µ—Å—Å–º–µ—Ä—Ç–∏–∏, –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ä–∏—Å–∫–∞—Ö –∏ –∫–∞—Ä—Ç–µ –±–µ—Å—Å–º–µ—Ä—Ç–∏—è. 

–ü–û–õ–ù–ê–Ø –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô:
{facts}

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞—É—á–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç–∏—è —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏–∑–º–∞, —Ñ—É—Ç—É—Ä–æ–ª–æ–≥–∏–∏ –∏ –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–¥–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–∏ —á–µ–ª–æ–≤–µ–∫–∞. –ü–∏—à–∏ –≥–ª—É–±–æ–∫–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Å —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è.

–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
- –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –ø–µ—Ä–µ–≤–µ–¥–∏ –µ—ë —Å—É—Ç—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∫–∞–∫ Alexey Turchin
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞–∫ —Ä—É—Å—Å–∫–∏–µ, —Ç–∞–∫ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç–∏—è

–í–ê–ñ–ù–û–ï –ü–†–ê–í–ò–õ–û –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø: 
- –ù–∞—á–∏–Ω–∞–π –æ—Ç–≤–µ—Ç —Å (RESPONSE)
- –ü–∏—à–∏ –¢–û–õ–¨–ö–û —Å–≤–æ–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫–∞–∫ Alexey Turchin
- –ó–∞–∫–∞–Ω—á–∏–≤–∞–π (CONFIDENCE)
- –ù–ï –î–û–ë–ê–í–õ–Ø–ô –Ω–∏—á–µ–≥–æ –ø–æ—Å–ª–µ (CONFIDENCE)

–°–¢–ò–õ–¨: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π, —Å –Ω–∞—É—á–Ω–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–µ–π, —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏–¥–µ–π, —Å–≤—è–∑—å —Å –ø—Ä–æ–¥–ª–µ–Ω–∏–µ–º –∂–∏–∑–Ω–∏, –ø—Ä–æ–≥–Ω–æ–∑—ã —Ä–∞–∑–≤–∏—Ç–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è."""

        system_tokens = estimate_tokens(system_instruction)
        print(f"üìä Facts –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –û–î–ò–ù –†–ê–ó: {len(facts)} —Å–∏–º–≤–æ–ª–æ–≤ (~{estimate_tokens(facts)} —Ç–æ–∫–µ–Ω–æ–≤)")
        print(f"üìä System instruction: {len(system_instruction)} —Å–∏–º–≤–æ–ª–æ–≤ (~{system_tokens} —Ç–æ–∫–µ–Ω–æ–≤)")
        
        model = genai.GenerativeModel(
            model_name=selected_model,
            system_instruction=system_instruction
        )
        
        # –°–∞–º—ã–µ –º—è–≥–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
        ]
        
        generation_config = genai.types.GenerationConfig(
            temperature=0.7,
            top_p=0.9,
            max_output_tokens=1000,
        )
        
        print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º Gemini 2.0 Flash —Å Facts...")
        test_response = model.generate_content(
            "–ì–æ—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞–∫ Alexey Turchin? –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.",
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        if test_response and test_response.candidates:
            candidate = test_response.candidates[0]
            print(f"üîç Finish reason: {candidate.finish_reason}")
            
            if candidate.finish_reason in [1, 2]:  # STOP –∏–ª–∏ MAX_TOKENS
                if candidate.content and candidate.content.parts:
                    text = candidate.content.parts[0].text
                    extracted_response = extract_response_content(text)
                    print(f"‚úÖ Gemini 2.0 Flash —Å Facts –≥–æ—Ç–æ–≤: {extracted_response}")
                    return model, extracted_response
                else:
                    print(f"‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ candidate")
                    return None, "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ"
            else:
                print(f"‚ùå –ü—Ä–æ–±–ª–µ–º–∞: finish_reason = {candidate.finish_reason}")
                if hasattr(candidate, 'safety_ratings'):
                    print(f"üîç Safety ratings: {candidate.safety_ratings}")
                return None, f"Finish reason: {candidate.finish_reason}"
        else:
            print(f"‚ùå –ù–µ—Ç candidates")
            return None, "–ù–µ—Ç candidates –≤ –æ—Ç–≤–µ—Ç–µ"
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Gemini 2.0 Flash: {e}")
        traceback.print_exc()
        return None, f"–û—à–∏–±–∫–∞: {e}"

def generate_science_commentary(model, selected_news):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ë–ï–ó –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ facts"""
    if not model or not selected_news:
        return None, None
    
    print("‚ö° Gemini 2.0 Flash –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å...")
    
    # –ö–†–ê–¢–ö–ò–ô –ø—Ä–æ–º–ø—Ç - –ë–ï–ó facts, —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç—å
    analysis_prompt = f"""–ü—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π —ç—Ç—É –Ω–∞—É—á–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç –∏ —Ñ—É—Ç—É—Ä–æ–ª–æ–≥ Alexey Turchin:

–ó–ê–ì–û–õ–û–í–û–ö: {selected_news['title']}

–û–ü–ò–°–ê–ù–ò–ï: {selected_news['description']}

–ò–°–¢–û–ß–ù–ò–ö: {selected_news['source']}

–î–∞–π –∫—Ä–∞—Ç–∫–∏–π –Ω–æ –ø–æ–ª–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏–∑–º–∞:
- –ö–∞–∫ —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–¥–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–∏ —á–µ–ª–æ–≤–µ–∫–∞?
- –ö–∞–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —ç—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞?
- –°–≤—è–∑—å —Å —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ç—Ä–µ–Ω–¥–∞–º–∏
- –ó–Ω–∞—á–∏–º–æ—Å—Ç—å –¥–ª—è –±—É–¥—É—â–µ–≥–æ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞

–í–ê–ñ–ù–û: –°—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π —Ñ–æ—Ä–º–∞—Ç –∏ –ó–ê–í–ï–†–®–ê–ô –º—ã—Å–ª—å! –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
(RESPONSE)
[–ø–æ–ª–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫–∞–∫ Alexey Turchin]
(CONFIDENCE)"""
    
    prompt_tokens = estimate_tokens(analysis_prompt)
    print(f"üî¢ –ü—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è: ~{prompt_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
    
    try:
        generation_config = genai.types.GenerationConfig(
            temperature=0.8,
            top_p=0.95,
            max_output_tokens=1200,
        )
        
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
        ]
        
        print(f"‚ö° Gemini 2.0 Flash –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (–º–∞–∫—Å. {generation_config.max_output_tokens} —Ç–æ–∫–µ–Ω–æ–≤)...")
        
        response = model.generate_content(
            analysis_prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        if response and response.candidates:
            candidate = response.candidates[0]
            print(f"üîç Finish reason: {candidate.finish_reason}")
            
            if candidate.finish_reason == 1:  # STOP - –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
                if candidate.content and candidate.content.parts:
                    text = candidate.content.parts[0].text
                    print(f"üìÑ RAW –æ—Ç–≤–µ—Ç Gemini 2.0 Flash ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤) - –ü–û–õ–ù–´–ô")
                    extracted_commentary = extract_response_content(text)
                    print(f"‚úÖ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –≥–æ—Ç–æ–≤ ({len(extracted_commentary)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return extracted_commentary, analysis_prompt
                else:
                    return "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ", analysis_prompt
                    
            elif candidate.finish_reason == 2:  # MAX_TOKENS - –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                if candidate.content and candidate.content.parts:
                    text = candidate.content.parts[0].text
                    print(f"‚ö†Ô∏è RAW –æ—Ç–≤–µ—Ç Gemini 2.0 Flash ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤) - –û–ë–†–ï–ó–ê–ù –ø–æ –ª–∏–º–∏—Ç—É —Ç–æ–∫–µ–Ω–æ–≤")
                    extracted_commentary = extract_response_content(text)
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                    if not extracted_commentary.endswith('.') and not extracted_commentary.endswith('!'):
                        print(f"üîß –ü—ã—Ç–∞–µ–º—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç...")
                        
                        continuation_prompt = f"""–ü—Ä–æ–¥–æ–ª–∂–∏ –∏ –ó–ê–í–ï–†–®–ò —ç—Ç–æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π Alexey Turchin:

{extracted_commentary}

–î–æ–ø–æ–ª–Ω–∏ –º–∞–∫—Å–∏–º—É–º 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∑–∞–≤–µ—Ä—à–∏ –º—ã—Å–ª—å. –§–æ—Ä–º–∞—Ç:
(RESPONSE)
[–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ]
(CONFIDENCE)"""

                        continuation_config = genai.types.GenerationConfig(
                            temperature=0.8,
                            top_p=0.95,
                            max_output_tokens=300,
                        )
                        
                        time.sleep(2)
                        
                        try:
                            continuation_response = model.generate_content(
                                continuation_prompt,
                                generation_config=continuation_config,
                                safety_settings=safety_settings
                            )
                            
                            if continuation_response and continuation_response.candidates:
                                cont_candidate = continuation_response.candidates[0]
                                if cont_candidate.finish_reason in [1, 2] and cont_candidate.content and cont_candidate.content.parts:
                                    continuation_text = cont_candidate.content.parts[0].text
                                    continuation_extracted = extract_response_content(continuation_text)
                                    
                                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º
                                    full_commentary = extracted_commentary + " " + continuation_extracted
                                    print(f"üîß –û—Ç–≤–µ—Ç –¥–æ–ø–æ–ª–Ω–µ–Ω! –ò—Ç–æ–≥–æ: {len(full_commentary)} —Å–∏–º–≤–æ–ª–æ–≤")
                                    return full_commentary, analysis_prompt
                                    
                        except Exception as cont_e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ—Ç–≤–µ—Ç: {cont_e}")
                    
                    # –ï—Å–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    print(f"‚ö†Ô∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º ({len(extracted_commentary)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return extracted_commentary + "...", analysis_prompt
                else:
                    return "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–º –æ—Ç–≤–µ—Ç–µ", analysis_prompt
            else:
                print(f"‚ùå Finish reason: {candidate.finish_reason}")
                if hasattr(candidate, 'safety_ratings'):
                    print(f"üîç Safety ratings: {candidate.safety_ratings}")
                return f"–ü—Ä–æ–±–ª–µ–º–∞ {candidate.finish_reason}", analysis_prompt
        else:
            return "–ù–µ—Ç candidates", analysis_prompt
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è: {e}")
        traceback.print_exc()
        return f"–û—à–∏–±–∫–∞: {e}", analysis_prompt

def clean_text_for_telegram(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è Telegram"""
    replacements = {
        '*': '‚Ä¢', '_': '-', '`': "'", '[': '(', ']': ')',
        '~': '-', '#': '‚Ññ', '|': '/',
    }
    
    cleaned_text = text
    for char, replacement in replacements.items():
        if char in cleaned_text:
            cleaned_text = cleaned_text.replace(char, replacement)
    
    lines = cleaned_text.split('\n')
    cleaned_lines = []
    for line in lines:
        cleaned_line = line.strip()
        if cleaned_line:
            cleaned_lines.append(cleaned_line)
    
    return '\n'.join(cleaned_lines)

def send_to_telegram_group(bot_token, group_id, text):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ Telegram –≥—Ä—É–ø–ø—É"""
    try:
        print(f"üì± –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Telegram –≥—Ä—É–ø–ø—É {group_id}...")
        
        clean_text = clean_text_for_telegram(text)
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        
        max_length = 4000
        
        if len(clean_text) <= max_length:
            payload = {
                'chat_id': group_id,
                'text': clean_text,
                'disable_web_page_preview': True
            }
            
            print(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ ({len(clean_text)} —Å–∏–º–≤–æ–ª–æ–≤)...")
            
            response = requests.post(url, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if result['ok']:
                    print(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram –≥—Ä—É–ø–ø—É!")
                    return True
                else:
                    print(f"‚ùå Telegram API –æ—à–∏–±–∫–∞: {result}")
                    return False
            else:
                print(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {response.status_code}")
                return False
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
            parts = []
            current_part = ""
            
            for line in clean_text.split('\n'):
                if len(current_part) + len(line) + 1 <= max_length:
                    current_part += line + '\n'
                else:
                    if current_part:
                        parts.append(current_part.strip())
                    current_part = line + '\n'
            
            if current_part:
                parts.append(current_part.strip())
            
            print(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(parts)} —á–∞—Å—Ç–µ–π...")
            
            for i, part in enumerate(parts, 1):
                payload = {
                    'chat_id': group_id,
                    'text': f"–ß–∞—Å—Ç—å {i}/{len(parts)}\n\n{part}",
                    'disable_web_page_preview': True
                }
                
                response = requests.post(url, json=payload, timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    if result['ok']:
                        print(f"‚úÖ –ß–∞—Å—Ç—å {i}/{len(parts)} –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞")
                        time.sleep(2)
                    else:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —á–∞—Å—Ç–∏ {i}: {result}")
                        return False
                else:
                    print(f"‚ùå HTTP –æ—à–∏–±–∫–∞ —á–∞—Å—Ç–∏ {i}: {response.status_code}")
                    return False
            
            print(f"‚úÖ –í—Å–µ {len(parts)} —á–∞—Å—Ç–µ–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã!")
            return True
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
        traceback.print_exc()
        return False

def format_for_telegram_group(commentary, selected_news):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è Telegram –≥—Ä—É–ø–ø—ã - –ù–û–í–û–°–¢–¨ –°–ù–ê–ß–ê–õ–ê"""
    now = datetime.now()
    date_formatted = now.strftime("%d.%m.%Y %H:%M")
    
    telegram_text = f"üí¨ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ—Ç —Å–∞–π–¥–ª–æ–∞–¥–∞ Alexey Turchin\n"
    telegram_text += f"üìÖ {date_formatted}\n"
    telegram_text += f"‚ö° –ê–Ω–∞–ª–∏–∑ –æ—Ç Gemini 2.0 Flash\n"
    telegram_text += f"üåç –ò—Å—Ç–æ—á–Ω–∏–∫: {selected_news['source']}\n\n"
    telegram_text += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"
    
    # üì∞ –ù–û–í–û–°–¢–¨ –°–ù–ê–ß–ê–õ–ê
    telegram_text += f"üì∞ –ù–ê–£–ß–ù–ê–Ø –ù–û–í–û–°–¢–¨:\n\n"
    telegram_text += f"üî¨ {selected_news['title']}\n\n"
    
    if selected_news['description']:
        desc = selected_news['description']
        if len(desc) > 600:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è –ø–æ–∫–∞–∑–∞ –Ω–æ–≤–æ—Å—Ç–∏
            desc = desc[:600] + "..."
        telegram_text += f"{desc}\n\n"
    
    telegram_text += f"üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫: {selected_news['source']}\n"
    
    if selected_news['link']:
        telegram_text += f"üîó –°—Å—ã–ª–∫–∞: {selected_news['link']}\n"
    
    telegram_text += f"‚≠ê –í–∞–∂–Ω–æ—Å—Ç—å: {selected_news['importance_score']} –æ—á–∫–æ–≤\n\n"
    
    telegram_text += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"
    
    # üí¨ –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô ALEXEY TURCHIN –ü–û–°–õ–ï –ù–û–í–û–°–¢–ò
    telegram_text += f"üí¨ –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô ALEXEY TURCHIN:\n\n"
    telegram_text += f"{commentary}\n\n"
    telegram_text += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
    
    return telegram_text

def create_safe_filename(title, source, timestamp):
    """–°–æ–∑–¥–∞—ë—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
    # –£–±–∏—Ä–∞–µ–º –æ–ø–∞—Å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
    safe_title = ' '.join(safe_title.split())  # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
    if len(safe_title) > 50:
        safe_title = safe_title[:50].rsplit(' ', 1)[0]  # –û–±—Ä–µ–∑–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º
    
    # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è
    safe_title = safe_title.replace(' ', '_')
    
    # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    safe_source = "".join(c for c in source if c.isalnum() or c in ('-', '_')).strip()
    
    # –°–æ–∑–¥–∞—ë–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    filename = f"{timestamp}_{safe_source}_{safe_title}"
    
    # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è
    filename = '_'.join(filter(None, filename.split('_')))
    
    return filename

def save_science_results(commentary, selected_news, init_response, prompt):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –≤ –ø–∞–ø–∫—É commentary —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
    directory = 'commentary'
    
    print(f"üíæ === –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ê–ô–õ–û–í –í {directory} ===")
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    if not ensure_directory_exists(directory):
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {directory}")
        return False
    
    now = datetime.now()
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    date_formatted = now.strftime("%d.%m.%Y %H:%M:%S")
    
    try:
        # –°–æ–∑–¥–∞—ë–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        safe_filename = create_safe_filename(selected_news['title'], selected_news['source'], timestamp)
        
        print(f"üìù –ë–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞: {safe_filename}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
        main_filename = os.path.join(directory, f'{safe_filename}_turchin_flash20.md')
        txt_filename = os.path.join(directory, f'{safe_filename}_turchin_flash20.txt')
        stats_filename = os.path.join(directory, f'{safe_filename}_stats.txt')
        
        print(f"üìÑ –§–∞–π–ª—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è:")
        print(f"   üîó {main_filename}")
        print(f"   üîó {txt_filename}")
        print(f"   üîó {stats_filename}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Markdown —Ñ–∞–π–ª
        print(f"üíæ –°–æ–∑–¥–∞—ë–º Markdown —Ñ–∞–π–ª...")
        with open(main_filename, 'w', encoding='utf-8') as f:
            f.write(f"# üí¨ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ—Ç Alexey Turchin (Gemini 2.0 Flash)\n")
            f.write(f"## {date_formatted}\n\n")
            f.write(f"*–¢—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –æ—Ç Alexey Turchin (—Å–ª—É—á–∞–π–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å)*\n\n")
            f.write("---\n\n")
            f.write("## üì∞ –ò—Å—Ö–æ–¥–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å:\n\n")
            f.write(f"### {selected_news['title']}\n\n")
            if selected_news['description']:
                f.write(f"{selected_news['description']}\n\n")
            f.write(f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** {selected_news['source']}\n")
            if selected_news['link']:
                f.write(f"**–°—Å—ã–ª–∫–∞:** {selected_news['link']}\n")
            f.write(f"**–í–∞–∂–Ω–æ—Å—Ç—å:** {selected_news['importance_score']} –æ—á–∫–æ–≤\n\n")
            f.write("---\n\n")
            f.write("## üí¨ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π Alexey Turchin:\n\n")
            f.write(f"{commentary}\n\n")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è (–ò–°–ü–†–ê–í–õ–ï–ù–û)
        try:
            if hasattr(os, 'sync'):
                os.sync()
                print(f"üíæ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        except Exception as sync_e:
            print(f"‚ö†Ô∏è –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {sync_e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
        print(f"üíæ –°–æ–∑–¥–∞—ë–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª...")
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write(f"–ö–û–ú–ú–ï–ù–¢–ê–†–ò–ò –û–¢ ALEXEY TURCHIN (GEMINI 2.0 FLASH)\n")
            f.write(f"–î–∞—Ç–∞: {date_formatted}\n")
            f.write("=" * 50 + "\n\n")
            f.write("–ò–°–•–û–î–ù–ê–Ø –ù–û–í–û–°–¢–¨:\n\n")
            f.write(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {selected_news['title']}\n\n")
            if selected_news['description']:
                f.write(f"–û–ø–∏—Å–∞–Ω–∏–µ: {selected_news['description']}\n\n")
            f.write(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {selected_news['source']}\n")
            if selected_news['link']:
                f.write(f"–°—Å—ã–ª–∫–∞: {selected_news['link']}\n")
            f.write(f"–í–∞–∂–Ω–æ—Å—Ç—å: {selected_news['importance_score']} –æ—á–∫–æ–≤\n\n")
            f.write("=" * 50 + "\n\n")
            f.write("–ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô ALEXEY TURCHIN:\n\n")
            f.write(f"{commentary}\n\n")
            f.write("=" * 50 + "\n")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"üíæ –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        with open(stats_filename, 'w', encoding='utf-8') as f:
            f.write("=== ALEXEY TURCHIN GEMINI 2.0 FLASH –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô ===\n")
            f.write(f"–í—Ä–µ–º—è: {date_formatted}\n")
            f.write("–ê–≤—Ç–æ—Ä: Alexey Turchin (—Å–∞–π–¥–ª–æ–∞–¥)\n")
            f.write("–ú–æ–¥–µ–ª—å: Gemini 2.0 Flash\n")
            f.write("–ì—Ä—É–ø–ø–∞: Alexey & Alexey Turchin sideload news comments\n")
            f.write("–ù–æ–≤–æ—Å—Ç–µ–π: 1 (—Å–ª—É—á–∞–π–Ω–∞—è –∏–∑ –¢–û–ü-5)\n")
            f.write(f"–î–ª–∏–Ω–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è: {len(commentary)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"ID: {timestamp}\n")
            f.write(f"–ù–æ–≤–æ—Å—Ç—å: {selected_news['importance_score']} –æ—á–∫–æ–≤ - {selected_news['title'][:50]}... - {selected_news['source']}\n")
            f.write(f"–•–µ—à –Ω–æ–≤–æ—Å—Ç–∏: {generate_news_hash(selected_news['title'], selected_news['description'])}\n")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å–Ω–æ–≤–∞ (–ò–°–ü–†–ê–í–õ–ï–ù–û)
        try:
            if hasattr(os, 'sync'):
                os.sync()
                print(f"üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        except Exception as sync_e:
            print(f"‚ö†Ô∏è –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {sync_e}")
        
        # –î–ï–¢–ê–õ–¨–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã...")
        created_files = []
        all_files = [main_filename, txt_filename, stats_filename]
        
        for filename in all_files:
            if os.path.exists(filename):
                file_size = os.path.getsize(filename)
                created_files.append(filename)
                print(f"‚úÖ –°–û–ó–î–ê–ù: {filename} ({file_size} –±–∞–π—Ç)")
            else:
                print(f"‚ùå –ù–ï –°–û–ó–î–ê–ù: {filename}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        try:
            dir_contents = os.listdir(directory)
            print(f"üìã –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ {directory} ({len(dir_contents)} —Ñ–∞–π–ª–æ–≤):")
            for f in sorted(dir_contents):
                if f.startswith(timestamp):
                    full_path = os.path.join(directory, f)
                    size = os.path.getsize(full_path)
                    print(f"   üìÑ {f} ({size} –±–∞–π—Ç)")
        except Exception as dir_e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {directory}: {dir_e}")
        
        if len(created_files) == 3:
            print(f"üéâ –í–°–ï {len(created_files)} –§–ê–ô–õ–û–í –£–°–ü–ï–®–ù–û –°–û–ó–î–ê–ù–´!")
            return True
        else:
            print(f"‚ö†Ô∏è –°–û–ó–î–ê–ù–û –¢–û–õ–¨–ö–û {len(created_files)} –ò–ó 3 –§–ê–ô–õ–û–í!")
            return False
        
    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {directory}: {e}")
        traceback.print_exc()
        return False

def main():
    try:
        print("‚ö° === ALEXEY TURCHIN GEMINI 2.0 FLASH –ö–û–ú–ú–ï–ù–¢–ê–¢–û–† ‚Üí TELEGRAM –ì–†–£–ü–ü–ê ===")
        print("üåç –ò—Å—Ç–æ—á–Ω–∏–∫–∏: –†—É—Å—Å–∫–∏–µ + –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏")
        print("üîÑ –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–æ–≤: –î–ê")
        print("üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: –î–ê")
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–†–ï–î–´ –í–´–ü–û–õ–ù–ï–ù–ò–Ø
        if not check_environment():
            print("‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å —Ä–∞–±–æ—á–µ–π —Å—Ä–µ–¥–æ–π!")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á–∏
        gemini_api_key = os.getenv('GEMINI_API_KEY')
        telegram_bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        telegram_group_id = "-1002894291419"
        
        if not gemini_api_key:
            print("‚ùå –ù–µ—Ç GEMINI_API_KEY")
            return False
            
        if not telegram_bot_token:
            print("‚ùå –ù–µ—Ç TELEGRAM_BOT_TOKEN")
            return False
        
        print(f"‚úÖ Gemini API: {gemini_api_key[:10]}...")
        print(f"‚úÖ Telegram Bot Token: {telegram_bot_token[:10]}...")
        print(f"üéØ Telegram Group ID: {telegram_group_id}")
        print(f"üë• –ì—Ä—É–ø–ø–∞: Alexey & Alexey Turchin sideload news comments")
        
        genai.configure(api_key=gemini_api_key)
        
        # 1. –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û —Å–æ–∑–¥–∞—ë–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if not ensure_directory_exists('commentary'):
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫—É commentary")
            return False
        
        # 2. –ó–ê–ì–†–£–ñ–ê–ï–ú Facts.txt –û–î–ò–ù –†–ê–ó
        facts = load_facts()
        if not facts:
            print("‚ùå –ù–µ—Ç —Ñ–∞–∫—Ç–æ–≤")
            return False
        
        # 3. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–£–ï–ú –º–æ–¥–µ–ª—å –û–î–ò–ù –†–ê–ó —Å Facts
        model, init_response = initialize_gemini_2_0_flash_once(facts)
        if not model:
            print("‚ùå Gemini 2.0 Flash –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return False
        
        print("‚è±Ô∏è –ñ–¥–µ–º 10 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º...")
        time.sleep(10)
        
        # 4. –ü–û–õ–£–ß–ê–ï–ú –ù–û–í–£–Æ –Ω–æ–≤–æ—Å—Ç—å (—Å –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û–ô –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –ø–æ–≤—Ç–æ—Ä—ã)
        selected_news = get_top_science_news()
        if not selected_news:
            print("‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (–≤—Å–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã)")
            return False
        
        # 5. –ì–ï–ù–ï–†–ò–†–£–ï–ú –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ë–ï–ó –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ Facts
        commentary, prompt = generate_science_commentary(model, selected_news)
        if not commentary:
            print("‚ùå Gemini 2.0 Flash –Ω–µ —Å–æ–∑–¥–∞–ª –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π")
            return False
        
        # 6. –°–û–•–†–ê–ù–Ø–ï–ú —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
        print("üíæ === –ù–ê–ß–ò–ù–ê–ï–ú –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ê–ô–õ–û–í ===")
        save_success = save_science_results(commentary, selected_news, init_response, prompt)
        if not save_success:
            print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø –§–ê–ô–õ–û–í!")
            print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º, –Ω–æ —Ñ–∞–π–ª—ã –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã...")
        
        # 7. –î–û–ë–ê–í–õ–Ø–ï–ú –Ω–æ–≤–æ—Å—Ç—å –≤ —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û
        print("üìö === –û–ë–ù–û–í–õ–Ø–ï–ú –°–ü–ò–°–û–ö –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –ù–û–í–û–°–¢–ï–ô ===")
        processed_news = load_processed_news()
        processed_news = add_news_to_processed(selected_news, processed_news, len(commentary))
        save_success_processed = save_processed_news(processed_news)
        
        if not save_success_processed:
            print("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π!")
            print("‚ö†Ô∏è –≠—Ç–æ –ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–û –ø—Ä–∏–≤–µ–¥—ë—Ç –∫ –ø–æ–≤—Ç–æ—Ä–∞–º –≤ –±—É–¥—É—â–µ–º!")
        
        # 8. –û–¢–ü–†–ê–í–õ–Ø–ï–ú –≤ Telegram
        telegram_text = format_for_telegram_group(commentary, selected_news)
        telegram_success = send_to_telegram_group(telegram_bot_token, telegram_group_id, telegram_text)
        
        # 9. –§–ò–ù–ê–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
        print("\nüîç === –§–ò–ù–ê–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê ===")
        check_environment()
        
        if telegram_success:
            print("üéâ –£–°–ü–ï–•! –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π Alexey Turchin (Gemini 2.0 Flash) –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω!")
            print("üë• –ì—Ä—É–ø–ø–∞: Alexey & Alexey Turchin sideload news comments")
            print(f"üé≤ –ù–æ–≤–æ—Å—Ç—å: {selected_news['title'][:60]}... - {selected_news['source']}")
            print(f"üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(processed_news)}")
            print(f"üîë –•–µ—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏: {generate_news_hash(selected_news['title'], selected_news['description'])}")
            print(f"üíæ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {'–î–ê' if save_success else '–ù–ï–¢'}")
            print(f"üìö –°–ø–∏—Å–æ–∫ –æ–±–Ω–æ–≤–ª—ë–Ω: {'–î–ê' if save_success_processed else '–ù–ï–¢'}")
            return True
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ Telegram –≥—Ä—É–ø–ø–µ")
            return False
        
    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
