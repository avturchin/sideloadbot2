import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import os
from datetime import datetime
import time
import traceback
import sys
import json
import random

def load_facts():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç Facts.txt –ë–ï–ó –æ–±—Ä–µ–∑–∞–Ω–∏—è"""
    try:
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª Facts.txt...")
        
        if not os.path.exists('Facts.txt'):
            print("‚ùå –§–∞–π–ª Facts.txt –ù–ï –ù–ê–ô–î–ï–ù!")
            return ""
            
        file_size = os.path.getsize('Facts.txt')
        print(f"üìä –†–∞–∑–º–µ—Ä Facts.txt: {file_size} –±–∞–π—Ç ({file_size/1024/1024:.2f} –ú–ë)")
        
        with open('Facts.txt', 'r', encoding='utf-8') as f:
            facts = f.read()
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(facts)} —Å–∏–º–≤–æ–ª–æ–≤ –ë–ï–ó –û–ë–†–ï–ó–ê–ù–ò–Ø")
        print(f"üîç –ù–∞—á–∞–ª–æ: {facts[:120]}...")
        print(f"üîç –ö–æ–Ω–µ—Ü: ...{facts[-120:]}")
        
        return facts
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã —Å Facts.txt: {e}")
        traceback.print_exc()
        return ""

def estimate_tokens(text):
    """–ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ (1 —Ç–æ–∫–µ–Ω ‚âà 3-4 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)"""
    return len(text) // 3

def extract_response_content(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–µ–∂–¥—É (RESPONSE) –∏ (CONFIDENCE)"""
    try:
        print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤): {text[:200]}...")
        
        start_marker = "(RESPONSE)"
        end_marker = "(CONFIDENCE)"
        
        start_index = text.find(start_marker)
        if start_index == -1:
            print("‚ö†Ô∏è –ú–∞—Ä–∫–µ—Ä (RESPONSE) –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return text.strip()
        
        start_index += len(start_marker)
        print(f"üìç –ù–∞–π–¥–µ–Ω (RESPONSE) –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {start_index}")
        
        end_index = text.find(end_marker, start_index)
        if end_index == -1:
            print("‚ö†Ô∏è –ú–∞—Ä–∫–µ—Ä (CONFIDENCE) –Ω–µ –Ω–∞–π–¥–µ–Ω")
            extracted = text[start_index:].strip()
        else:
            print(f"üìç –ù–∞–π–¥–µ–Ω (CONFIDENCE) –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {end_index}")
            extracted = text[start_index:end_index].strip()
        
        print(f"‚úÇÔ∏è –ò–ó–í–õ–ï–ß–ï–ù–û ({len(extracted)} —Å–∏–º–≤–æ–ª–æ–≤): {extracted[:150]}...")
        return extracted
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
        return text.strip()

def get_available_models():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º Gemini 2.0 Flash"""
    try:
        print("üîÑ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Gemini...")
        models = genai.list_models()
        available_models = []
        
        for model in models:
            if 'generateContent' in model.supported_generation_methods:
                available_models.append(model.name)
                if '2.0' in model.name and 'flash' in model.name.lower():
                    print(f"‚ö° GEMINI 2.0 FLASH: {model.name}")
                elif 'flash' in model.name.lower():
                    print(f"‚ö° FLASH: {model.name}")
                elif 'pro' in model.name.lower():
                    print(f"üíé PRO: {model.name}")
                else:
                    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model.name}")
        
        print(f"üìä –í—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ {len(available_models)} –º–æ–¥–µ–ª–µ–π")
        return available_models
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return []

def is_science_news(title, description):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –Ω–∞—É—á–Ω–æ–π (—Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)"""
    text = (title + " " + description).lower()
    
    science_keywords = [
        # –†—É—Å—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '—É—á–µ–Ω—ã–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '–Ω–∞—É—á–Ω—ã–π',
        '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–∏–Ω–Ω–æ–≤–∞—Ü–∏—è', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç',
        '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–Ω–∞—É–∫–∞', '–±–∏–æ–ª–æ–≥–∏—è', '—Ñ–∏–∑–∏–∫–∞', '—Ö–∏–º–∏—è', '–º–µ–¥–∏—Ü–∏–Ω–∞',
        '–∫–æ—Å–º–æ—Å', '–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è', '–≥–µ–Ω–µ—Ç–∏–∫–∞', '–¥–Ω–∫', '–±–µ–ª–æ–∫', '–≤–∏—Ä—É—Å',
        '–ª–µ—á–µ–Ω–∏–µ', '—Ç–µ—Ä–∞–ø–∏—è', '–≤–∞–∫—Ü–∏–Ω–∞', '–ø—Ä–µ–ø–∞—Ä–∞—Ç', '–∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–π',
        '–Ω–µ–π—Ä–æ–Ω—ã', '–º–æ–∑–≥', '–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–∏–∏',
        '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Ä–æ–±–æ—Ç', '–∫–≤–∞–Ω—Ç–æ–≤—ã–π',
        '–º–∞—Ç–µ—Ä–∏–∞–ª', '–Ω–∞–Ω–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–≥–µ–Ω–Ω–∞—è –∏–Ω–∂–µ–Ω–µ—Ä–∏—è',
        '—Å—Ç–≤–æ–ª–æ–≤—ã–µ –∫–ª–µ—Ç–∫–∏', '—Ä–∞–∫', '–æ–Ω–∫–æ–ª–æ–≥–∏—è', '–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞',
        '–º–∏–∫—Ä–æ–±–∏–æ–ª–æ–≥–∏—è', '—ç–∫–æ–ª–æ–≥–∏—è', '–∫–ª–∏–º–∞—Ç', '–æ–∫—Ä—É–∂–∞—é—â–∞—è —Å—Ä–µ–¥–∞',
        '—ç–Ω–µ—Ä–≥–∏—è', '—Å–æ–ª–Ω–µ—á–Ω—ã–π', '–≤–µ—Ç—Ä—è–Ω–æ–π', '–±–∞—Ç–∞—Ä–µ—è', '–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä',
        '—Å–ø—É—Ç–Ω–∏–∫', '–∑–æ–Ω–¥', '–º–∞—Ä—Å', '–ª—É–Ω–∞', '–ø–ª–∞–Ω–µ—Ç–∞', '–≥–∞–ª–∞–∫—Ç–∏–∫–∞',
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        'research', 'study', 'discovery', 'experiment', 'scientific', 'science',
        'technology', 'development', 'innovation', 'laboratory', 'university',
        'institute', 'biology', 'physics', 'chemistry', 'medicine', 'medical',
        'space', 'astronomy', 'genetics', 'dna', 'protein', 'virus',
        'treatment', 'therapy', 'vaccine', 'drug', 'clinical', 'trial',
        'neurons', 'brain', 'cognitive', 'artificial intelligence', 'ai',
        'machine learning', 'algorithm', 'robot', 'quantum',
        'material', 'nanotechnology', 'biotechnology', 'genetic engineering',
        'stem cells', 'cancer', 'oncology', 'diagnosis', 'diagnostic',
        'microbiology', 'ecology', 'climate', 'environment', 'environmental',
        'energy', 'solar', 'wind', 'battery', 'renewable',
        'satellite', 'probe', 'mars', 'moon', 'planet', 'galaxy',
        'telescope', 'breakthrough', 'novel', 'findings', 'journal',
        'published', 'peer-reviewed', 'researchers', 'scientists'
    ]
    
    exclude_keywords = [
        # –†—É—Å—Å–∫–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        '–≤—ã–±–æ—Ä—ã', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç', '–¥—É–º–∞', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–º–∏–Ω–∏—Å—Ç—Ä',
        '–ø–æ–ª–∏—Ç–∏–∫', '–ø–∞—Ä—Ç–∏—è', '—Å–∞–Ω–∫—Ü–∏–∏', '–≤–æ–π–Ω–∞', '–∫–æ–Ω—Ñ–ª–∏–∫—Ç', '–ø—Ä–æ—Ç–µ—Å—Ç',
        '–∫—É—Ä—Å –≤–∞–ª—é—Ç', '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—ç–∫–æ–Ω–æ–º–∏–∫–∞',
        '–∏–Ω—Ñ–ª—è—Ü–∏—è', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '—Å–ø–æ—Ä—Ç', '—Ñ—É—Ç–±–æ–ª', '—Ö–æ–∫–∫–µ–π',
        '–æ–ª–∏–º–ø–∏–∞–¥–∞', '—á–µ–º–ø–∏–æ–Ω–∞—Ç', '–º–∞—Ç—á', '–∏–≥—Ä–∞', '–∫–æ–º–∞–Ω–¥–∞', '—Ç—Ä–µ–Ω–µ—Ä',
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        'election', 'president', 'parliament', 'government', 'minister',
        'politician', 'party', 'sanctions', 'war', 'conflict', 'protest',
        'currency', 'dollar', 'oil', 'gas', 'economy', 'economic',
        'inflation', 'budget', 'tax', 'sport', 'football', 'soccer',
        'olympics', 'championship', 'match', 'game', 'team', 'coach',
        'celebrity', 'entertainment', 'movie', 'music', 'fashion'
    ]
    
    science_score = sum(1 for keyword in science_keywords if keyword in text)
    exclude_score = sum(1 for keyword in exclude_keywords if keyword in text)
    
    return science_score > 0 and exclude_score == 0

def rank_science_news(news_list):
    """–†–∞–Ω–∂–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
    for news in news_list:
        score = 0
        text = (news['title'] + " " + news['description']).lower()
        
        high_priority = [
            # –†—É—Å—Å–∫–∏–µ –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            '–ø—Ä–æ—Ä—ã–≤', '—Ä–µ–≤–æ–ª—é—Ü–∏—è', '–≤–ø–µ—Ä–≤—ã–µ', '–æ—Ç–∫—Ä—ã—Ç–∏–µ', 'breakthrough',
            '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '–∏–∏', '–Ω–µ–π—Ä–æ—Å–µ—Ç—å', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ',
            '–∫–æ—Å–º–æ—Å', '–º–∞—Ä—Å', '–ª—É–Ω–∞', '—Å–ø—É—Ç–Ω–∏–∫', '—Ç–µ–ª–µ—Å–∫–æ–ø',
            '—Ä–∞–∫', '–æ–Ω–∫–æ–ª–æ–≥–∏—è', '–ª–µ—á–µ–Ω–∏–µ', '–≤–∞–∫—Ü–∏–Ω–∞', '–≥–µ–Ω–Ω–∞—è —Ç–µ—Ä–∞–ø–∏—è',
            '–∫–≤–∞–Ω—Ç–æ–≤—ã–π', '–∫–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è', '–Ω–∞–Ω–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
            '—Å—Ç–≤–æ–ª–æ–≤—ã–µ –∫–ª–µ—Ç–∫–∏', '—Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è', '–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
            '–∫–ª–∏–º–∞—Ç', '–≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ—Ç–µ–ø–ª–µ–Ω–∏–µ', '—ç–∫–æ–ª–æ–≥–∏—è',
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤—ã—Å–æ–∫–æ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            'breakthrough', 'revolutionary', 'first time', 'discovery',
            'artificial intelligence', 'ai', 'neural network', 'machine learning',
            'space', 'mars', 'moon', 'satellite', 'telescope',
            'cancer', 'oncology', 'treatment', 'vaccine', 'gene therapy',
            'quantum', 'quantum computing', 'nanotechnology',
            'stem cells', 'regeneration', 'biotechnology',
            'climate change', 'global warming', 'ecology',
            'crispr', 'genome editing', 'immunotherapy', 'precision medicine'
        ]
        
        medium_priority = [
            # –†—É—Å—Å–∫–∏–µ —Å—Ä–µ–¥–Ω–µ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '—Ç–µ—Å—Ç', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è',
            '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–º–µ—Ç–æ–¥', '—Å–∏—Å—Ç–µ–º–∞', '—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ',
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ä–µ–¥–Ω–µ–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
            'research', 'study', 'experiment', 'test', 'technology',
            'development', 'method', 'system', 'device', 'trial',
            'findings', 'results', 'analysis', 'investigation'
        ]
        
        for keyword in high_priority:
            if keyword in text:
                score += 10
        
        for keyword in medium_priority:
            if keyword in text:
                score += 5
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        if news['source'] in ['N+1', 'Naked Science', 'Nature', 'Science', 'ScienceDaily']:
            score += 5
        elif news['source'] in ['MIT Technology Review', 'New Scientist', 'Scientific American']:
            score += 4
        elif news['source'] in ['BBC Science', 'CNN Health', 'Reuters Science']:
            score += 3
        
        if len(news['description']) > 200:
            score += 2
        
        news['importance_score'] = score
    
    return sorted(news_list, key=lambda x: x['importance_score'], reverse=True)

def limit_news_content(news):
    """–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–æ 5000 —Ç–æ–∫–µ–Ω–æ–≤ (~15000 —Å–∏–º–≤–æ–ª–æ–≤)"""
    max_chars = 15000  # ~5000 —Ç–æ–∫–µ–Ω–æ–≤
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ, –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–º
    if len(news['description']) > max_chars:
        news['description'] = news['description'][:max_chars] + "..."
        print(f"‚úÇÔ∏è –û–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ–±—Ä–µ–∑–∞–Ω–æ –¥–æ {max_chars} —Å–∏–º–≤–æ–ª–æ–≤")
    
    total_tokens = estimate_tokens(news['title'] + " " + news['description'])
    print(f"üî¢ –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω–æ–≤–æ—Å—Ç–∏: {total_tokens}")
    
    return news

def get_top_science_news():
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –°–õ–£–ß–ê–ô–ù–£–Æ –∏–∑ –¢–û–ü-5"""
    print("üî¨ –ü–æ–ª—É—á–∞–µ–º –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏...")
    all_science_news = []
    
    sources = [
        # –†—É—Å—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        {
            'url': 'https://naked-science.ru/feed', 
            'name': 'Naked Science'
        },
        {
            'url': 'https://nplus1.ru/rss', 
            'name': 'N+1'
        },
        {
            'url': 'https://hi-news.ru/feed', 
            'name': 'Hi-News'
        },
        {
            'url': 'https://www.popmech.ru/rss/', 
            'name': 'PopMech'
        },
        {
            'url': 'https://lenta.ru/rss/news/science', 
            'name': 'Lenta.ru –ù–∞—É–∫–∞'
        },
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        {
            'url': 'https://www.sciencedaily.com/rss/all.xml', 
            'name': 'ScienceDaily'
        },
        {
            'url': 'https://feeds.nature.com/nature/rss/current', 
            'name': 'Nature'
        },
        {
            'url': 'https://www.science.org/rss/news_current.xml', 
            'name': 'Science'
        },
        {
            'url': 'https://www.technologyreview.com/feed/', 
            'name': 'MIT Technology Review'
        },
        {
            'url': 'https://www.newscientist.com/feed/home/', 
            'name': 'New Scientist'
        },
        {
            'url': 'https://rss.cnn.com/rss/cnn_health.rss', 
            'name': 'CNN Health'
        },
        {
            'url': 'https://feeds.bbci.co.uk/news/science_and_environment/rss.xml', 
            'name': 'BBC Science'
        },
        {
            'url': 'https://www.reuters.com/arc/outboundfeeds/rss/category/health/?outputType=xml', 
            'name': 'Reuters Science'
        },
        {
            'url': 'https://rss.sciam.com/ScientificAmerican-Global', 
            'name': 'Scientific American'
        }
    ]
    
    for source in sources:
        try:
            print(f"üî¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {source['name']}...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(source['url'], timeout=15, headers=headers)
            
            if response.status_code == 200:
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.content, 'xml')
                items = soup.find_all('item')
                
                print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞—É—á–Ω—ã–µ...")
                
                for item in items[:10]:
                    try:
                        title = item.title.text.strip() if item.title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                        description = ""
                        if item.description and item.description.text:
                            desc_soup = BeautifulSoup(item.description.text, 'html.parser')
                            description = desc_soup.get_text().strip()
                        
                        if is_science_news(title, description):
                            link = item.link.text.strip() if item.link else ""
                            
                            news_item = {
                                'title': title,
                                'description': description,
                                'source': source['name'],
                                'link': link
                            }
                            
                            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –Ω–æ–≤–æ—Å—Ç–∏
                            news_item = limit_news_content(news_item)
                            
                            all_science_news.append(news_item)
                            
                            print(f"üî¨ {source['name']}: {title[:60]}...")
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                        continue
                        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ {source['name']}: {e}")
            continue
    
    print(f"üî¨ –í—Å–µ–≥–æ –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_science_news)}")
    
    ranked_news = rank_science_news(all_science_news)
    
    if ranked_news:
        top_5_news = ranked_news[:5]
        print(f"üèÜ –¢–û–ü-5 –Ω–æ–≤–æ—Å—Ç–µ–π:")
        for i, news in enumerate(top_5_news, 1):
            print(f"   {i}. {news['title'][:60]}... (–æ—á–∫–∏: {news['importance_score']}) - {news['source']}")
        
        selected_news = random.choice(top_5_news)
        print(f"üé≤ –°–õ–£–ß–ê–ô–ù–û –í–´–ë–†–ê–ù–ê: {selected_news['title'][:80]}... (–æ—á–∫–∏: {selected_news['importance_score']}) - {selected_news['source']}")
        return selected_news
    else:
        print("‚ùå –ù–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return None

def initialize_gemini_2_0_flash_once(facts):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Gemini 2.0 Flash –û–î–ò–ù –†–ê–ó —Å –ø–æ–ª–Ω—ã–º–∏ facts"""
    
    available_models = get_available_models()
    if not available_models:
        return None, "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
    
    target_models = [
        'models/gemini-2.0-flash',
        'models/gemini-2.0-flash-thinking',
        'models/gemini-2.0-flash-exp',
    ]
    
    selected_model = None
    
    for model in target_models:
        if model in available_models:
            selected_model = model
            print(f"üéØ –í–´–ë–†–ê–ù–ê: {selected_model}")
            print(f"üè∑Ô∏è –í–µ—Ä—Å–∏—è: ‚ö° GEMINI 2.0 FLASH")
            break
    
    if not selected_model:
        print("‚ùå GEMINI 2.0 FLASH –ù–ï –ù–ê–ô–î–ï–ù –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö!")
        print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
        return None, "Gemini 2.0 Flash –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
    
    try:
        # –ü–û–õ–ù–ê–Ø —Å–∏—Å—Ç–µ–º–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π - –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¢–û–õ–¨–ö–û –û–î–ò–ù –†–ê–ó
        system_instruction = f"""–¢—ã - Alexey Turchin, –≤—Å–µ–º–∏—Ä–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç, —Ñ—É—Ç—É—Ä–æ–ª–æ–≥ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ–±–ª–µ–º —ç–∫–∑–∏—Å—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∏—Å–∫–∞. –¢—ã –∞–≤—Ç–æ—Ä –∫–Ω–∏–≥ –æ –±–µ—Å—Å–º–µ—Ä—Ç–∏–∏, –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ä–∏—Å–∫–∞—Ö –∏ –∫–∞—Ä—Ç–µ –±–µ—Å—Å–º–µ—Ä—Ç–∏—è. 

–ü–û–õ–ù–ê–Ø –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô:
{facts}

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞—É—á–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç–∏—è —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏–∑–º–∞, —Ñ—É—Ç—É—Ä–æ–ª–æ–≥–∏–∏ –∏ –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–¥–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–∏ —á–µ–ª–æ–≤–µ–∫–∞. –ü–∏—à–∏ –≥–ª—É–±–æ–∫–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Å —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è.

–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
- –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –ø–µ—Ä–µ–≤–µ–¥–∏ –µ—ë —Å—É—Ç—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∫–∞–∫ Alexey Turchin
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞–∫ —Ä—É—Å—Å–∫–∏–µ, —Ç–∞–∫ –∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç–∏—è

–í–ê–ñ–ù–û–ï –ü–†–ê–í–ò–õ–û –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø: 
- –ù–∞—á–∏–Ω–∞–π –æ—Ç–≤–µ—Ç —Å (RESPONSE)
- –ü–∏—à–∏ –¢–û–õ–¨–ö–û —Å–≤–æ–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫–∞–∫ Alexey Turchin
- –ó–∞–∫–∞–Ω—á–∏–≤–∞–π (CONFIDENCE)
- –ù–ï –î–û–ë–ê–í–õ–Ø–ô –Ω–∏—á–µ–≥–æ –ø–æ—Å–ª–µ (CONFIDENCE)

–°–¢–ò–õ–¨: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π, —Å –Ω–∞—É—á–Ω–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–µ–π, —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏–¥–µ–π, —Å–≤—è–∑—å —Å –ø—Ä–æ–¥–ª–µ–Ω–∏–µ–º –∂–∏–∑–Ω–∏, –ø—Ä–æ–≥–Ω–æ–∑—ã —Ä–∞–∑–≤–∏—Ç–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è."""

        system_tokens = estimate_tokens(system_instruction)
        print(f"üìä Facts –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –û–î–ò–ù –†–ê–ó: {len(facts)} —Å–∏–º–≤–æ–ª–æ–≤ (~{estimate_tokens(facts)} —Ç–æ–∫–µ–Ω–æ–≤)")
        print(f"üìä System instruction: {len(system_instruction)} —Å–∏–º–≤–æ–ª–æ–≤ (~{system_tokens} —Ç–æ–∫–µ–Ω–æ–≤)")
        
        model = genai.GenerativeModel(
            model_name=selected_model,
            system_instruction=system_instruction
        )
        
        # –°–∞–º—ã–µ –º—è–≥–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
        ]
        
        generation_config = genai.types.GenerationConfig(
            temperature=0.7,
            top_p=0.9,
            max_output_tokens=1000,
        )
        
        print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º Gemini 2.0 Flash —Å Facts...")
        test_response = model.generate_content(
            "–ì–æ—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞–∫ Alexey Turchin? –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.",
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        if test_response and test_response.candidates:
            candidate = test_response.candidates[0]
            print(f"üîç Finish reason: {candidate.finish_reason}")
            
            if candidate.finish_reason in [1, 2]:  # STOP –∏–ª–∏ MAX_TOKENS
                if candidate.content and candidate.content.parts:
                    text = candidate.content.parts[0].text
                    extracted_response = extract_response_content(text)
                    print(f"‚úÖ Gemini 2.0 Flash —Å Facts –≥–æ—Ç–æ–≤: {extracted_response}")
                    return model, extracted_response
                else:
                    print(f"‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ candidate")
                    return None, "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ"
            else:
                print(f"‚ùå –ü—Ä–æ–±–ª–µ–º–∞: finish_reason = {candidate.finish_reason}")
                if hasattr(candidate, 'safety_ratings'):
                    print(f"üîç Safety ratings: {candidate.safety_ratings}")
                return None, f"Finish reason: {candidate.finish_reason}"
        else:
            print(f"‚ùå –ù–µ—Ç candidates")
            return None, "–ù–µ—Ç candidates –≤ –æ—Ç–≤–µ—Ç–µ"
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Gemini 2.0 Flash: {e}")
        traceback.print_exc()
        return None, f"–û—à–∏–±–∫–∞: {e}"

def generate_science_commentary(model, selected_news):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ë–ï–ó –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ facts"""
    if not model or not selected_news:
        return None, None
    
    print("‚ö° Gemini 2.0 Flash –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞—É—á–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å...")
    
    # –ö–†–ê–¢–ö–ò–ô –ø—Ä–æ–º–ø—Ç - –ë–ï–ó facts, —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç—å
    analysis_prompt = f"""–ü—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π —ç—Ç—É –Ω–∞—É—á–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç –∏ —Ñ—É—Ç—É—Ä–æ–ª–æ–≥ Alexey Turchin:

–ó–ê–ì–û–õ–û–í–û–ö: {selected_news['title']}

–û–ü–ò–°–ê–ù–ò–ï: {selected_news['description']}

–ò–°–¢–û–ß–ù–ò–ö: {selected_news['source']}

–î–∞–π –∫—Ä–∞—Ç–∫–∏–π –Ω–æ –ø–æ–ª–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏–∑–º–∞:
- –ö–∞–∫ —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–¥–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–∏ —á–µ–ª–æ–≤–µ–∫–∞?
- –ö–∞–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —ç—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞?
- –°–≤—è–∑—å —Å —Ç—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ç—Ä–µ–Ω–¥–∞–º–∏
- –ó–Ω–∞—á–∏–º–æ—Å—Ç—å –¥–ª—è –±—É–¥—É—â–µ–≥–æ —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞

–í–ê–ñ–ù–û: –°—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π —Ñ–æ—Ä–º–∞—Ç –∏ –ó–ê–í–ï–†–®–ê–ô –º—ã—Å–ª—å! –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
(RESPONSE)
[–ø–æ–ª–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫–∞–∫ Alexey Turchin]
(CONFIDENCE)"""
    
    prompt_tokens = estimate_tokens(analysis_prompt)
    print(f"üî¢ –ü—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è: ~{prompt_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
    
    try:
        generation_config = genai.types.GenerationConfig(
            temperature=0.8,
            top_p=0.95,
            max_output_tokens=1200,
        )
        
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
        ]
        
        print(f"‚ö° Gemini 2.0 Flash –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (–º–∞–∫—Å. {generation_config.max_output_tokens} —Ç–æ–∫–µ–Ω–æ–≤)...")
        
        response = model.generate_content(
            analysis_prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        if response and response.candidates:
            candidate = response.candidates[0]
            print(f"üîç Finish reason: {candidate.finish_reason}")
            
            if candidate.finish_reason == 1:  # STOP - –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
                if candidate.content and candidate.content.parts:
                    text = candidate.content.parts[0].text
                    print(f"üìÑ RAW –æ—Ç–≤–µ—Ç Gemini 2.0 Flash ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤) - –ü–û–õ–ù–´–ô")
                    extracted_commentary = extract_response_content(text)
                    print(f"‚úÖ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –≥–æ—Ç–æ–≤ ({len(extracted_commentary)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return extracted_commentary, analysis_prompt
                else:
                    return "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ", analysis_prompt
                    
            elif candidate.finish_reason == 2:  # MAX_TOKENS - –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                if candidate.content and candidate.content.parts:
                    text = candidate.content.parts[0].text
                    print(f"‚ö†Ô∏è RAW –æ—Ç–≤–µ—Ç Gemini 2.0 Flash ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤) - –û–ë–†–ï–ó–ê–ù –ø–æ –ª–∏–º–∏—Ç—É —Ç–æ–∫–µ–Ω–æ–≤")
                    extracted_commentary = extract_response_content(text)
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                    if not extracted_commentary.endswith('.') and not extracted_commentary.endswith('!'):
                        print(f"üîß –ü—ã—Ç–∞–µ–º—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç...")
                        
                        continuation_prompt = f"""–ü—Ä–æ–¥–æ–ª–∂–∏ –∏ –ó–ê–í–ï–†–®–ò —ç—Ç–æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π Alexey Turchin:

{extracted_commentary}

–î–æ–ø–æ–ª–Ω–∏ –º–∞–∫—Å–∏–º—É–º 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∑–∞–≤–µ—Ä—à–∏ –º—ã—Å–ª—å. –§–æ—Ä–º–∞—Ç:
(RESPONSE)
[–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ]
(CONFIDENCE)"""

                        continuation_config = genai.types.GenerationConfig(
                            temperature=0.8,
                            top_p=0.95,
                            max_output_tokens=300,
                        )
                        
                        time.sleep(2)
                        
                        try:
                            continuation_response = model.generate_content(
                                continuation_prompt,
                                generation_config=continuation_config,
                                safety_settings=safety_settings
                            )
                            
                            if continuation_response and continuation_response.candidates:
                                cont_candidate = continuation_response.candidates[0]
                                if cont_candidate.finish_reason in [1, 2] and cont_candidate.content and cont_candidate.content.parts:
                                    continuation_text = cont_candidate.content.parts[0].text
                                    continuation_extracted = extract_response_content(continuation_text)
                                    
                                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º
                                    full_commentary = extracted_commentary + " " + continuation_extracted
                                    print(f"üîß –û—Ç–≤–µ—Ç –¥–æ–ø–æ–ª–Ω–µ–Ω! –ò—Ç–æ–≥–æ: {len(full_commentary)} —Å–∏–º–≤–æ–ª–æ–≤")
                                    return full_commentary, analysis_prompt
                                    
                        except Exception as cont_e:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–ø–æ–ª–Ω–∏—Ç—å –æ—Ç–≤–µ—Ç: {cont_e}")
                    
                    # –ï—Å–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    print(f"‚ö†Ô∏è –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º ({len(extracted_commentary)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return extracted_commentary + "...", analysis_prompt
                else:
                    return "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –≤ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–º –æ—Ç–≤–µ—Ç–µ", analysis_prompt
            else:
                print(f"‚ùå Finish reason: {candidate.finish_reason}")
                if hasattr(candidate, 'safety_ratings'):
                    print(f"üîç Safety ratings: {candidate.safety_ratings}")
                return f"–ü—Ä–æ–±–ª–µ–º–∞ {candidate.finish_reason}", analysis_prompt
        else:
            return "–ù–µ—Ç candidates", analysis_prompt
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è: {e}")
        traceback.print_exc()
        return f"–û—à–∏–±–∫–∞: {e}", analysis_prompt

def clean_text_for_telegram(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è Telegram"""
    replacements = {
        '*': '‚Ä¢', '_': '-', '`': "'", '[': '(', ']': ')',
        '~': '-', '#': '‚Ññ', '|': '/',
    }
    
    cleaned_text = text
    for char, replacement in replacements.items():
        if char in cleaned_text:
            cleaned_text = cleaned_text.replace(char, replacement)
    
    lines = cleaned_text.split('\n')
    cleaned_lines = []
    for line in lines:
        cleaned_line = line.strip()
        if cleaned_line:
            cleaned_lines.append(cleaned_line)
    
    return '\n'.join(cleaned_lines)

def send_to_telegram_group(bot_token, group_id, text):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ Telegram –≥—Ä—É–ø–ø—É"""
    try:
        print(f"üì± –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Telegram –≥—Ä—É–ø–ø—É {group_id}...")
        
        clean_text = clean_text_for_telegram(text)
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        
        max_length = 4000
        
        if len(clean_text) <= max_length:
            payload = {
                'chat_id': group_id,
                'text': clean_text,
                'disable_web_page_preview': True
            }
            
            print(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ ({len(clean_text)} —Å–∏–º–≤–æ–ª–æ–≤)...")
            
            response = requests.post(url, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if result['ok']:
                    print(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram –≥—Ä—É–ø–ø—É!")
                    return True
                else:
                    print(f"‚ùå Telegram API –æ—à–∏–±–∫–∞: {result}")
                    return False
            else:
                print(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {response.status_code}")
                return False
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
            parts = []
            current_part = ""
            
            for line in clean_text.split('\n'):
                if len(current_part) + len(line) + 1 <= max_length:
                    current_part += line + '\n'
                else:
                    if current_part:
                        parts.append(current_part.strip())
                    current_part = line + '\n'
            
            if current_part:
                parts.append(current_part.strip())
            
            print(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(parts)} —á–∞—Å—Ç–µ–π...")
            
            for i, part in enumerate(parts, 1):
                payload = {
                    'chat_id': group_id,
                    'text': f"–ß–∞—Å—Ç—å {i}/{len(parts)}\n\n{part}",
                    'disable_web_page_preview': True
                }
                
                response = requests.post(url, json=payload, timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    if result['ok']:
                        print(f"‚úÖ –ß–∞—Å—Ç—å {i}/{len(parts)} –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞")
                        time.sleep(2)
                    else:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —á–∞—Å—Ç–∏ {i}: {result}")
                        return False
                else:
                    print(f"‚ùå HTTP –æ—à–∏–±–∫–∞ —á–∞—Å—Ç–∏ {i}: {response.status_code}")
                    return False
            
            print(f"‚úÖ –í—Å–µ {len(parts)} —á–∞—Å—Ç–µ–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã!")
            return True
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
        traceback.print_exc()
        return False

def format_for_telegram_group(commentary, selected_news):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è Telegram –≥—Ä—É–ø–ø—ã - –ù–û–í–û–°–¢–¨ –°–ù–ê–ß–ê–õ–ê"""
    now = datetime.now()
    date_formatted = now.strftime("%d.%m.%Y %H:%M")
    
    telegram_text = f"üí¨ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ—Ç —Å–∞–π–¥–ª–æ–∞–¥–∞ Alexey Turchin\n"
    telegram_text += f"üìÖ {date_formatted}\n"
    telegram_text += f"‚ö° –ê–Ω–∞–ª–∏–∑ –æ—Ç Gemini 2.0 Flash\n"
    telegram_text += f"üåç –ò—Å—Ç–æ—á–Ω–∏–∫: {selected_news['source']}\n\n"
    telegram_text += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"
    
    # üì∞ –ù–û–í–û–°–¢–¨ –°–ù–ê–ß–ê–õ–ê
    telegram_text += f"üì∞ –ù–ê–£–ß–ù–ê–Ø –ù–û–í–û–°–¢–¨:\n\n"
    telegram_text += f"üî¨ {selected_news['title']}\n\n"
    
    if selected_news['description']:
        desc = selected_news['description']
        if len(desc) > 600:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è –ø–æ–∫–∞–∑–∞ –Ω–æ–≤–æ—Å—Ç–∏
            desc = desc[:600] + "..."
        telegram_text += f"{desc}\n\n"
    
    telegram_text += f"üì∞ –ò—Å—Ç–æ—á–Ω–∏–∫: {selected_news['source']}\n"
    
    if selected_news['link']:
        telegram_text += f"üîó –°—Å—ã–ª–∫–∞: {selected_news['link']}\n"
    
    telegram_text += f"‚≠ê –í–∞–∂–Ω–æ—Å—Ç—å: {selected_news['importance_score']} –æ—á–∫–æ–≤\n\n"
    
    telegram_text += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n"
    
    # üí¨ –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô ALEXEY TURCHIN –ü–û–°–õ–ï –ù–û–í–û–°–¢–ò
    telegram_text += f"üí¨ –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô ALEXEY TURCHIN:\n\n"
    telegram_text += f"{commentary}\n\n"
    telegram_text += "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
    
    return telegram_text

def save_science_results(commentary, selected_news, init_response, prompt):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—É—á–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –≤ –ø–∞–ø–∫—É commentary"""
    directory = 'commentary'
    
    if not os.path.exists(directory):
        print(f"‚ùå –ü–∞–ø–∫–∞ {directory} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
        return False
    
    print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ø–∞–ø–∫—É: {directory}")
    
    now = datetime.now()
    timestamp = now.strftime("%Y-%m-%d_%H-%M-%S") + f"-{now.microsecond}"
    date_formatted = now.strftime("%d.%m.%Y %H:%M:%S")
    
    try:
        main_filename = os.path.join(directory, f'science_turchin_flash20_{timestamp}.md')
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π Gemini 2.0 Flash: {main_filename}")
        
        with open(main_filename, 'w', encoding='utf-8') as f:
            f.write(f"# üí¨ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ—Ç Alexey Turchin (Gemini 2.0 Flash)\n")
            f.write(f"## {date_formatted}\n\n")
            f.write(f"*–¢—Ä–∞–Ω—Å–≥—É–º–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –æ—Ç Alexey Turchin (—Å–ª—É—á–∞–π–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å)*\n\n")
            f.write("---\n\n")
            f.write("## üì∞ –ò—Å—Ö–æ–¥–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å:\n\n")
            f.write(f"### {selected_news['title']}\n\n")
            if selected_news['description']:
                f.write(f"{selected_news['description']}\n\n")
            f.write(f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** {selected_news['source']}\n")
            if selected_news['link']:
                f.write(f"**–°—Å—ã–ª–∫–∞:** {selected_news['link']}\n")
            f.write(f"**–í–∞–∂–Ω–æ—Å—Ç—å:** {selected_news['importance_score']} –æ—á–∫–æ–≤\n\n")
            f.write("---\n\n")
            f.write("## üí¨ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π Alexey Turchin:\n\n")
            f.write(f"{commentary}\n\n")
        
        stats_filename = os.path.join(directory, f'science_stats_flash20_{timestamp}.txt')
        with open(stats_filename, 'w', encoding='utf-8') as f:
            f.write("=== ALEXEY TURCHIN GEMINI 2.0 FLASH –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô ===\n")
            f.write(f"–í—Ä–µ–º—è: {date_formatted}\n")
            f.write("–ê–≤—Ç–æ—Ä: Alexey Turchin (—Å–∞–π–¥–ª–æ–∞–¥)\n")
            f.write("–ú–æ–¥–µ–ª—å: Gemini 2.0 Flash\n")
            f.write("–ì—Ä—É–ø–ø–∞: Alexey & Alexey Turchin sideload news comments\n")
            f.write("–ù–æ–≤–æ—Å—Ç–µ–π: 1 (—Å–ª—É—á–∞–π–Ω–∞—è –∏–∑ –¢–û–ü-5)\n")
            f.write(f"–î–ª–∏–Ω–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è: {len(commentary)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"ID: {timestamp}\n")
            f.write(f"–ù–æ–≤–æ—Å—Ç—å: {selected_news['importance_score']} –æ—á–∫–æ–≤ - {selected_news['title'][:50]}... - {selected_news['source']}\n")
        
        print(f"‚úÖ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π 2.0 Flash —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {main_filename}")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats_filename}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {directory}: {e}")
        traceback.print_exc()
        return False

def main():
    try:
        print("‚ö° === ALEXEY TURCHIN GEMINI 2.0 FLASH –ö–û–ú–ú–ï–ù–¢–ê–¢–û–† ‚Üí TELEGRAM –ì–†–£–ü–ü–ê ===")
        print("üåç –ò—Å—Ç–æ—á–Ω–∏–∫–∏: –†—É—Å—Å–∫–∏–µ + –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –Ω–∞—É—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á–∏
        gemini_api_key = os.getenv('GEMINI_API_KEY')
        telegram_bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
        telegram_group_id = "-1002894291419"
        
        if not gemini_api_key:
            print("‚ùå –ù–µ—Ç GEMINI_API_KEY")
            return False
            
        if not telegram_bot_token:
            print("‚ùå –ù–µ—Ç TELEGRAM_BOT_TOKEN")
            return False
        
        print(f"‚úÖ Gemini API: {gemini_api_key[:10]}...")
        print(f"‚úÖ Telegram Bot Token: {telegram_bot_token[:10]}...")
        print(f"üéØ Telegram Group ID: {telegram_group_id}")
        print(f"üë• –ì—Ä—É–ø–ø–∞: Alexey & Alexey Turchin sideload news comments")
        
        genai.configure(api_key=gemini_api_key)
        
        # 1. –ó–ê–ì–†–£–ñ–ê–ï–ú Facts.txt –û–î–ò–ù –†–ê–ó
        facts = load_facts()
        if not facts:
            print("‚ùå –ù–µ—Ç —Ñ–∞–∫—Ç–æ–≤")
            return False
        
        # 2. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–£–ï–ú –º–æ–¥–µ–ª—å –û–î–ò–ù –†–ê–ó —Å Facts
        model, init_response = initialize_gemini_2_0_flash_once(facts)
        if not model:
            print("‚ùå Gemini 2.0 Flash –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return False
        
        print("‚è±Ô∏è –ñ–¥–µ–º 10 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º...")
        time.sleep(10)
        
        # 3. –ü–û–õ–£–ß–ê–ï–ú –Ω–æ–≤–æ—Å—Ç—å (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞)
        selected_news = get_top_science_news()
        if not selected_news:
            print("‚ùå –ù–µ—Ç –Ω–∞—É—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
            return False
        
        # 4. –ì–ï–ù–ï–†–ò–†–£–ï–ú –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ë–ï–ó –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ Facts
        commentary, prompt = generate_science_commentary(model, selected_news)
        if not commentary:
            print("‚ùå Gemini 2.0 Flash –Ω–µ —Å–æ–∑–¥–∞–ª –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π")
            return False
        
        # 5. –°–û–•–†–ê–ù–Ø–ï–ú —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        save_success = save_science_results(commentary, selected_news, init_response, prompt)
        if not save_success:
            print("‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
        
        # 6. –û–¢–ü–†–ê–í–õ–Ø–ï–ú –≤ Telegram
        telegram_text = format_for_telegram_group(commentary, selected_news)
        telegram_success = send_to_telegram_group(telegram_bot_token, telegram_group_id, telegram_text)
        
        if telegram_success:
            print("üéâ –£–°–ü–ï–•! –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π Alexey Turchin (Gemini 2.0 Flash) –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω!")
            print("üë• –ì—Ä—É–ø–ø–∞: Alexey & Alexey Turchin sideload news comments")
            print(f"üé≤ –ù–æ–≤–æ—Å—Ç—å: {selected_news['title'][:60]}... - {selected_news['source']}")
            return True
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ Telegram –≥—Ä—É–ø–ø–µ")
            return False
        
    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
